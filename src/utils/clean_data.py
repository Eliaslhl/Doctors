"""
Module de nettoyage des donn√©es de vaccination.

Ce module transforme les donn√©es brutes (rawdata.csv) en donn√©es nettoy√©es (cleaneddata.csv)
en appliquant plusieurs √©tapes de nettoyage et de validation.
"""

from typing import Optional
from pathlib import Path
import pandas as pd
import numpy as np

from config import RAW_DATA_DIR, CLEANED_DATA_DIR


def clean_vaccination_data(
    input_file: Optional[Path] = None,
    output_file: Optional[Path] = None
) -> pd.DataFrame:
    """
    Nettoie les donn√©es de vaccination brutes.
    
    √âtapes de nettoyage :
    1. Suppression des lignes avec valeurs manquantes critiques
    2. Conversion des types de donn√©es
    3. Suppression des doublons
    4. Normalisation des valeurs de couverture
    5. Validation des ann√©es
    6. Nettoyage des noms de pays
    
    Args:
        input_file: Chemin du fichier d'entr√©e (d√©faut: data/raw/rawdata.csv)
        output_file: Chemin du fichier de sortie (d√©faut: data/cleaned/cleaneddata.csv)
        
    Returns:
        DataFrame contenant les donn√©es nettoy√©es
        
    Raises:
        FileNotFoundError: Si le fichier d'entr√©e n'existe pas
        ValueError: Si les donn√©es sont invalides
    """
    # D√©finir les chemins par d√©faut
    if input_file is None:
        input_file = RAW_DATA_DIR / "rawdata.csv"
    if output_file is None:
        output_file = CLEANED_DATA_DIR / "cleaneddata.csv"
    
    if not input_file.exists():
        raise FileNotFoundError(f"Le fichier {input_file} n'existe pas")
    
    print(f"üìÇ Chargement des donn√©es depuis {input_file}...")
    data = pd.read_csv(input_file)
    initial_rows = len(data)
    print(f"‚úì {initial_rows} enregistrements charg√©s")
    print(f"\nüìã Colonnes d√©tect√©es: {list(data.columns)}")
    
    # Supprime les lignes avec valeurs manquantes critiques
    print("\nüßπ √âtape 1: Suppression des valeurs manquantes critiques...")
    critical_columns = ['GROUP', 'CODE', 'NAME', 'YEAR', 'ANTIGEN', 'COVERAGE']
    data_clean = data.dropna(subset=critical_columns)
    removed_missing = initial_rows - len(data_clean)
    print(f"‚úì {removed_missing} lignes supprim√©es")
    
    # Convertit les types de donn√©es
    print("\nüîÑ √âtape 2: Conversion des types de donn√©es...")
    data_clean['YEAR'] = pd.to_numeric(data_clean['YEAR'], errors='coerce')
    data_clean['COVERAGE'] = pd.to_numeric(data_clean['COVERAGE'], errors='coerce')
    data_clean = data_clean.dropna(subset=['YEAR', 'COVERAGE'])
    data_clean['YEAR'] = data_clean['YEAR'].astype(int)
    print(f"‚úì Types convertis (YEAR: int, COVERAGE: float)")
    
    # Valide les ann√©es (1980-2025)
    print("\nüìÖ √âtape 3: Validation des ann√©es...")
    current_year = 2025
    min_year = 1980
    data_clean = data_clean[
        (data_clean['YEAR'] >= min_year) & 
        (data_clean['YEAR'] <= current_year)
    ]
    year_range = f"{data_clean['YEAR'].min()} - {data_clean['YEAR'].max()}"
    print(f"‚úì Ann√©es valides: {year_range}")
    
    # Normalise la couverture (0-100%)
    print("\nüìä √âtape 4: Normalisation de la couverture...")
    data_clean.loc[data_clean['COVERAGE'] < 0, 'COVERAGE'] = 0
    data_clean.loc[data_clean['COVERAGE'] > 100, 'COVERAGE'] = 100
    coverage_stats = f"{data_clean['COVERAGE'].min():.1f}% - {data_clean['COVERAGE'].max():.1f}%"
    print(f"‚úì Couverture normalis√©e: {coverage_stats}")
    
    # Nettoie les textes
    print("\nüî§ √âtape 5: Nettoyage des textes...")
    text_columns = ['GROUP', 'CODE', 'NAME', 'ANTIGEN', 'COVERAGE_CATEGORY']
    for col in text_columns:
        if col in data_clean.columns:
            data_clean[col] = data_clean[col].astype(str).str.strip()
            data_clean[col] = data_clean[col].replace('', np.nan)
    print(f"‚úì {len(text_columns)} colonnes nettoy√©es")
    
    # Supprime les doublons
    print("\nüîç √âtape 6: Suppression des doublons...")
    before_dedup = len(data_clean)
    duplicate_cols = ['CODE', 'NAME', 'YEAR', 'ANTIGEN', 'COVERAGE_CATEGORY']
    data_clean = data_clean.drop_duplicates(subset=duplicate_cols, keep='first')
    duplicates_removed = before_dedup - len(data_clean)
    print(f"‚úì {duplicates_removed} doublons supprim√©s")
    
    # Trie les donn√©es
    print("\nüìë √âtape 7: Tri des donn√©es...")
    data_clean = data_clean.sort_values(
        by=['NAME', 'YEAR', 'ANTIGEN'],
        ascending=[True, True, True]
    )
    print(f"‚úì Donn√©es tri√©es (NAME ‚Üí YEAR ‚Üí ANTIGEN)")
    
    data_clean = data_clean.reset_index(drop=True)
    
    final_rows = len(data_clean)
    retention_rate = (final_rows / initial_rows) * 100
    
    print("\n" + "="*60)
    print("üìä R√âSUM√â DU NETTOYAGE")
    print("="*60)
    print(f"Enregistrements initiaux : {initial_rows}")
    print(f"Enregistrements finaux   : {final_rows}")
    print(f"Supprim√©s                : {initial_rows - final_rows}")
    print(f"Taux de r√©tention        : {retention_rate:.1f}%")
    print(f"\nPays uniques             : {data_clean['NAME'].nunique()}")
    print(f"Ann√©es                   : {data_clean['YEAR'].nunique()}")
    print(f"Antig√®nes                : {data_clean['ANTIGEN'].nunique()}")
    print(f"Couverture moyenne       : {data_clean['COVERAGE'].mean():.2f}%")
    print("="*60)
    
    # Sauvegarde les donn√©es nettoy√©es
    print(f"\nüíæ Sauvegarde dans {output_file}...")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    data_clean.to_csv(output_file, index=False)
    print(f"‚úì Donn√©es sauvegard√©es!")
    
    return data_clean


def validate_cleaned_data(data: pd.DataFrame) -> bool:
    """Valide les donn√©es nettoy√©es."""
    print("\nüîç Validation des donn√©es...")
    
    issues = []
    
    # V√©rifie les colonnes requises
    required_columns = ['GROUP', 'CODE', 'NAME', 'YEAR', 'ANTIGEN', 'COVERAGE']
    for col in required_columns:
        if col not in data.columns:
            issues.append(f"‚ùå Colonne manquante: {col}")
    
    # V√©rifie les valeurs manquantes
    for col in required_columns:
        if col in data.columns:
            missing = data[col].isna().sum()
            if missing > 0:
                issues.append(f"‚ö†Ô∏è  {missing} valeurs manquantes dans {col}")
    
    # V√©rifie les plages de valeurs
    if 'COVERAGE' in data.columns:
        if (data['COVERAGE'] < 0).any() or (data['COVERAGE'] > 100).any():
            issues.append("‚ùå Couverture hors plage (0-100%)")
    
    if 'YEAR' in data.columns:
        if (data['YEAR'] < 1980).any() or (data['YEAR'] > 2025).any():
            issues.append("‚ùå Ann√©es invalides")
    
    # Affiche les r√©sultats
    if issues:
        print("\n‚ö†Ô∏è  Probl√®mes d√©tect√©s:")
        for issue in issues:
            print(f"  {issue}")
        return False
    else:
        print("‚úÖ Validation r√©ussie")
        return True


def get_data_quality_report(data: pd.DataFrame) -> dict:
    """G√©n√®re un rapport de qualit√© des donn√©es."""
    missing_counts = data.isna().sum()
    completeness = (1 - (missing_counts / len(data))) * 100
    
    report = {
        'total_records': len(data),
        'countries': data['NAME'].nunique() if 'NAME' in data.columns else 0,
        'years': data['YEAR'].nunique() if 'YEAR' in data.columns else 0,
        'antigens': data['ANTIGEN'].nunique() if 'ANTIGEN' in data.columns else 0,
        'coverage_mean': data['COVERAGE'].mean() if 'COVERAGE' in data.columns else 0,
        'coverage_median': data['COVERAGE'].median() if 'COVERAGE' in data.columns else 0,
        'coverage_std': data['COVERAGE'].std() if 'COVERAGE' in data.columns else 0,
        'missing_values': missing_counts.to_dict(),
        'data_completeness': completeness.to_dict()
    }
    
    return report


if __name__ == "__main__":
    """Script principal de nettoyage des donn√©es."""
    print("üè• NETTOYAGE DES DONN√âES DE VACCINATION")
    print("="*60)
    
    try:
        # Nettoie les donn√©es
        cleaned_data = clean_vaccination_data()
        
        # Valide les donn√©es
        is_valid = validate_cleaned_data(cleaned_data)
        
        # G√©n√®re le rapport de qualit√©
        report = get_data_quality_report(cleaned_data)
        
        print("\nüìà RAPPORT DE QUALIT√â")
        print("="*60)
        print(f"Compl√©tude moyenne: {np.mean(list(report['data_completeness'].values())):.1f}%")
        
        if is_valid:
            print("\n‚úÖ Nettoyage termin√© avec succ√®s!")
        else:
            print("\n‚ö†Ô∏è  Nettoyage termin√© avec des avertissements")
            
    except Exception as e:
        print(f"\n‚ùå Erreur lors du nettoyage: {e}")
        raise
